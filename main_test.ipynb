{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabriellima/Documents/CentraleSupelec/Mention/ApprAutomatique/Project/Welding-Quality-Prediction/imputer.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.numeric_features] = X[self.numeric_features].apply(pd.to_numeric, errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler # it is not affected by outliers.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, learning_curve, KFold, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score, make_scorer\n",
    "\n",
    "from sklearn.feature_selection import f_regression, SelectKBest, RFECV\n",
    "# from imputer import create_full_pipeline\n",
    "from imputer import create_full_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "\n",
    "data_original = pd.read_csv('Assets/Data/welddb.csv', delimiter='\\s+', header=None)\n",
    "\n",
    "data = data_original.copy().replace({\"N\": np.nan})\n",
    "\n",
    "# Name the columns\n",
    "data.columns = [\n",
    "    'Carbon concentration (weight%)', \n",
    "    'Silicon concentration (weight%)', \n",
    "    'Manganese concentration (weight%)', \n",
    "    'Sulphur concentration (weight%)', \n",
    "    'Phosphorus concentration (weight%)', \n",
    "    'Nickel concentration (weight%)', \n",
    "    'Chromium concentration (weight%)', \n",
    "    'Molybdenum concentration (weight%)', \n",
    "    'Vanadium concentration (weight%)', \n",
    "    'Copper concentration (weight%)', \n",
    "    'Cobalt concentration (weight%)', \n",
    "    'Tungsten concentration (weight%)', \n",
    "    'Oxygen concentration (ppm by weight)', \n",
    "    'Titanium concentration (ppm by weight)', \n",
    "    'Nitrogen concentration (ppm by weight)', \n",
    "    'Aluminium concentration (ppm by weight)', \n",
    "    'Boron concentration (ppm by weight)', \n",
    "    'Niobium concentration (ppm by weight)', \n",
    "    'Tin concentration (ppm by weight)', \n",
    "    'Arsenic concentration (ppm by weight)', \n",
    "    'Antimony concentration (ppm by weight)', \n",
    "    'Current (A)', \n",
    "    'Voltage (V)', \n",
    "    'AC or DC', \n",
    "    'Electrode positive or negative', \n",
    "    'Heat input (kJ/mm)', \n",
    "    'Interpass temperature (°C)', \n",
    "    'Type of weld', \n",
    "    'Post weld heat treatment temperature (°C)', \n",
    "    'Post weld heat treatment time (hours)', \n",
    "    'Yield strength (MPa)', \n",
    "    'Ultimate tensile strength (MPa)', \n",
    "    'Elongation (%)', \n",
    "    'Reduction of Area (%)', \n",
    "    'Charpy temperature (°C)', \n",
    "    'Charpy impact toughness (J)', \n",
    "    'Hardness (kg/mm2)', \n",
    "    '50% FATT', \n",
    "    'Primary ferrite in microstructure (%)', \n",
    "    'Ferrite with second phase (%)', \n",
    "    'Acicular ferrite (%)', \n",
    "    'Martensite (%)', \n",
    "    'Ferrite with carbide aggregate (%)', \n",
    "    'Weld ID'\n",
    "]\n",
    "\n",
    "# Definição das colunas categóricas e numéricas\n",
    "categoric_features = ['AC or DC', 'Electrode positive or negative', 'Type of weld']\n",
    "numeric_features = ['Sulphur concentration (weight%)', 'Nickel concentration (weight%)', \n",
    "                    'Silicon concentration (weight%)', 'Phosphorus concentration (weight%)', \n",
    "                    'Titanium concentration (ppm by weight)', 'Nitrogen concentration (ppm by weight)', \n",
    "                    'Oxygen concentration (ppm by weight)', 'Voltage (V)', 'Heat input (kJ/mm)']\n",
    "\n",
    "# Separação dos dados em treino e teste\n",
    "X = data.drop(columns = [\"Yield strength (MPa)\", \"Weld ID\"])\n",
    "y = data[\"Yield strength (MPa)\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Criando o pipeline com as features necessárias\n",
    "full_pipeline = create_full_pipeline()\n",
    "\n",
    "# Dropping missing values in test\n",
    "X_test_clean = X_test[~y_test.isna()]\n",
    "y_test_clean = y_test.dropna().astype(float)\n",
    "\n",
    "# Aplicando o pipeline nos dados de treino\n",
    "X_train_transformed = X_train.copy()\n",
    "X_train_transformed = full_pipeline.fit_transform(X_train_transformed)\n",
    "\n",
    "# Aplicando o pipeline nos dados de teste\n",
    "X_test_transformed = full_pipeline.transform(X_test_clean)\n",
    "\n",
    "# hotencoder = HotEncoderCategorical(X_train_transformed)\n",
    "# X_train_transformed = hotencoder.fit_transform(X_train_transformed)\n",
    "# X_test_transformed = hotencoder.transform(X_test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um modelo base (nesse caso, um RandomForest)\n",
    "# base_model = RandomForestClassifier()\n",
    "base_model = LabelSpreading(kernel='knn', n_neighbors=7)\n",
    "\n",
    "# Criar o modelo de auto-treinamento\n",
    "self_training_model = SelfTrainingClassifier(base_model)\n",
    "\n",
    "# X_train contém todas as variáveis explicativas\n",
    "# y_train contém os labels, mas com valores NaN para os rótulos faltantes\n",
    "# Importante: O SelfTrainingClassifier trata os valores faltantes como -1, então convertemos NaN para -1\n",
    "y_train_imputed = y_train.fillna(-1)\n",
    "\n",
    "# Treinar o modelo de auto-treinamento\n",
    "self_training_model.fit(X_train_transformed, y_train_imputed)\n",
    "\n",
    "\n",
    "# Preencher somente os valores faltantes no y_train\n",
    "missing_mask = y_train.isna()\n",
    "\n",
    "# Assegurar que os valores previstos sejam do tipo float\n",
    "y_predicted = self_training_model.predict(X_train_transformed).astype(float)\n",
    "\n",
    "# Apenas preencher os valores que estavam como NaN no conjunto original\n",
    "y_train_completed = y_train.copy()\n",
    "\n",
    "# Preencher somente os valores faltantes no y_train\n",
    "missing_mask = y_train.isna()\n",
    "y_train_completed[missing_mask] = y_predicted[missing_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Categoric features \n",
    "\n",
    "# ## Linear Regression\n",
    "# def print_metrics(y_pred_lr, y_test_clean):\n",
    "#     print(\"MAPE :\", mean_absolute_percentage_error(y_pred_lr, y_test_clean))\n",
    "#     print(\"R2 :\", r2_score(y_pred_lr, y_test_clean))\n",
    "#     print(\"MSE :\", mean_squared_error(y_pred_lr, y_test_clean))\n",
    "\n",
    "# def train_test_models(X_train_transformed, y_train_completed):\n",
    "\n",
    "#     print(\"\\n -------------------- Linear Regression -------------------- \\n\")\n",
    "#     lr_pipeline = Pipeline(\n",
    "#         [\n",
    "#             (\"Regressor\", LinearRegression())\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     lr_pipeline.fit(X_train_transformed, y_train_completed)\n",
    "#     y_pred_lr = lr_pipeline.predict(X_test_transformed)\n",
    "#     print_metrics(y_pred_lr, y_test_clean)\n",
    "\n",
    "#     ## Ridge Regression\n",
    "\n",
    "#     print(\"\\n -------------------- Ridge Regression -------------------- \\n\")\n",
    "#     ridge_pipeline = Pipeline(\n",
    "#         [\n",
    "#             (\"Ridge Regressor\", Ridge())\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     ridge_pipeline.fit(X_train_transformed, y_train_completed)\n",
    "#     y_pred_ridge = ridge_pipeline.predict(X_test_transformed)\n",
    "#     print_metrics(y_pred_ridge, y_test_clean)\n",
    "\n",
    "#     ## Lasso Regression\n",
    "\n",
    "#     print(\"\\n -------------------- Lasso Regression -------------------- \\n\")\n",
    "#     lasso_pipeline = Pipeline(\n",
    "#         [\n",
    "#             (\"Lasso Regressor\", Lasso())\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     lasso_pipeline.fit(X_train_transformed, y_train_completed)\n",
    "#     y_pred_lasso = lasso_pipeline.predict(X_test_transformed)\n",
    "\n",
    "#     print_metrics(y_pred_lasso, y_test_clean)\n",
    "\n",
    "#     ## ElasticNet Regression\n",
    "\n",
    "#     print(\"\\n -------------------- ElasticNet Regression -------------------- \\n\")\n",
    "#     ElasticNet_pipeline = Pipeline(\n",
    "#         [\n",
    "#             (\"Regressor\", ElasticNet())\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     ElasticNet_pipeline.fit(X_train_transformed, y_train_completed)\n",
    "#     y_pred_ElasticNet = ElasticNet_pipeline.predict(X_test_transformed)\n",
    "\n",
    "#     print_metrics(y_pred_ElasticNet, y_test_clean)\n",
    "\n",
    "#     ## Decision Tree Regression\n",
    "\n",
    "#     print(\"\\n -------------------- Decision Tree Regression -------------------- \\n\")\n",
    "#     tree_pipeline = Pipeline(\n",
    "#         [\n",
    "#             (\"Regressor\", DecisionTreeRegressor())\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     tree_pipeline.fit(X_train_transformed, y_train_completed)\n",
    "#     y_pred_tree = tree_pipeline.predict(X_test_transformed)\n",
    "\n",
    "#     print_metrics(y_pred_tree, y_test_clean)\n",
    "\n",
    "#     ## Random Forest Regression\n",
    "\n",
    "#     print(\"\\n -------------------- Random Forest Regression -------------------- \\n\")\n",
    "#     RF_pipeline = Pipeline(\n",
    "#         [\n",
    "#             (\"Regressor\", RandomForestRegressor())\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     RF_pipeline.fit(X_train_transformed, y_train_completed)\n",
    "#     y_pred_RF = RF_pipeline.predict(X_test_transformed)\n",
    "\n",
    "#     print_metrics(y_pred_RF, y_test_clean)\n",
    "\n",
    "#     ## Gradient Boosting Regression\n",
    "\n",
    "#     print(\"\\n -------------------- Gradient Boosting Regression -------------------- \\n\")\n",
    "#     gb_pipeline = Pipeline(\n",
    "#         [\n",
    "#             (\"Regressor\", GradientBoostingRegressor())\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     gb_pipeline.fit(X_train_transformed, y_train_completed)\n",
    "#     y_pred_gb = gb_pipeline.predict(X_test_transformed)\n",
    "\n",
    "#     print_metrics(y_pred_gb, y_test_clean)\n",
    "\n",
    "# train_test_models(X_train_transformed, y_train_completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1028.9364518465923\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "model.fit(X_train_transformed, y_train_completed)\n",
    "\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "\n",
    "print(mean_squared_error(y_test_clean, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando o modelo XGB...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .................................................... total time=  47.3s\n",
      "[CV] END .................................................... total time=  48.4s\n",
      "[CV] END .................................................... total time=  48.8s\n",
      "[CV] END .................................................... total time=  48.9s\n",
      "[CV] END .................................................... total time=  49.0s\n",
      "Melhor modelo para XGB: {}\n",
      "Erro quadrado médio no teste para XGB: 1172.3513182383833\n",
      "Treinando o modelo DecisionTree...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .................................................... total time=   2.7s\n",
      "[CV] END .................................................... total time=   3.0s\n",
      "[CV] END .................................................... total time=   3.2s\n",
      "[CV] END .................................................... total time=   3.2s\n",
      "[CV] END .................................................... total time=   2.8s\n",
      "Melhor modelo para DecisionTree: {}\n",
      "Erro quadrado médio no teste para DecisionTree: 1945.8599180327867\n",
      "Treinando o modelo GradientBoosting...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .................................................... total time= 1.3min\n",
      "[CV] END .................................................... total time= 1.3min\n",
      "[CV] END .................................................... total time= 1.3min\n",
      "[CV] END .................................................... total time= 1.3min\n",
      "[CV] END .................................................... total time= 1.3min\n",
      "Melhor modelo para GradientBoosting: {}\n",
      "Erro quadrado médio no teste para GradientBoosting: 1743.8687639787486\n"
     ]
    }
   ],
   "source": [
    "# Criar o Pipeline para cada modelo\n",
    "def create_pipeline(estimator):\n",
    "    return Pipeline(steps=[\n",
    "        ('feature_selection', RFECV(estimator=estimator, step=1, cv=KFold(5), scoring='neg_mean_squared_error')),\n",
    "        ('regressor', estimator)\n",
    "    ])\n",
    "\n",
    "# Definir os modelos de regressão\n",
    "models = {\n",
    "    # 'RandomForest': RandomForestRegressor(random_state=42),\n",
    "    'XGB': xgb.XGBRegressor(random_state=42),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Parâmetros para a busca em grid (ajuste conforme necessário)\n",
    "param_grid = {\n",
    "    'pipeline__feature_selection__min_features_to_select': [5, 10, 15],\n",
    "    \n",
    "    'pipeline__regressor__n_estimators': [100, 200],  # Para RandomForest e GradientBoosting\n",
    "    'pipeline__regressor__max_depth': [3, 5, 10],     # Para DecisionTree e GradientBoosting\n",
    "    'pipeline__regressor__learning_rate': [0.01, 0.1, 0.2],  # Para GradientBoosting\n",
    "    'pipeline__regressor__n_estimators': [100, 200],  # Para XGB\n",
    "}\n",
    "\n",
    "# Scoring para regressão (Mean Squared Error)\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# Dicionário para armazenar os melhores modelos\n",
    "best_models = {}\n",
    "\n",
    "# Fazer a busca em grid para cada modelo\n",
    "for name, model in models.items():\n",
    "    print(f\"Treinando o modelo {name}...\")\n",
    "    \n",
    "    # Criar o pipeline para o modelo\n",
    "    pipeline = create_pipeline(model)\n",
    "    \n",
    "    # Fazer a busca em grid para encontrar os melhores parâmetros\n",
    "    grid_search = GridSearchCV(pipeline, param_grid={f'pipeline__regressor__{param}': value for param, value in param_grid.items() if param.startswith(name)},\n",
    "                               scoring=scorer, cv=5, verbose=2, n_jobs=-1)\n",
    "    \n",
    "    # Ajustar o modelo\n",
    "    grid_search.fit(X_train_transformed, y_train_completed)\n",
    "    \n",
    "    # Guardar o melhor modelo e seus parâmetros\n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    print(f\"Melhor modelo para {name}: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Fazer previsões nos dados de teste\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test_transformed)\n",
    "    \n",
    "    # Avaliar o modelo no conjunto de teste\n",
    "    test_mse = mean_squared_error(y_test_clean, y_pred)\n",
    "    print(f\"Erro quadrado médio no teste para {name}: {test_mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
